{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"![earthmover](https://edanalytics.github.io/earthmover/assets/ea-earthmover.png)  <p><code>earthmover</code> transforms collections of tabular source data (flat files, FTP files, database tables/queries) into text-based (JSONL, XML) data via YAML configuration.</p>  ![earthmover demo](https://edanalytics.github.io/earthmover/assets/earthmover_demo.gif)"},{"location":"#quick-start","title":"Quick-start","text":"<ol> <li> <p>Install <code>earthmover</code> with     <pre><code>pip install earthmover\n</code></pre></p> </li> <li> <p>Create an <code>earthmover.yml</code> configuration file that defines your project config, data sources, transformations, and destinations:     earthmover.yml<pre><code>version: 2\n\nconfig:\n  output_dir: ./output/\n\nsources:\n  csv_source:\n    file: ./data/file.csv\n    header_rows: 1\n  sql_source:\n    connection: \"postgresql://user:pass@host/database\"\n    query: &gt;\n      select column1, column2, column3\n      from myschema.mytable\n\ntransformations:\n  stacked:\n    source: $sources.csv_source\n    operations:\n      - operation: union\n        sources:\n          - $sources.sql_source\n\ndestinations:\n  mydata:\n    source: $transformations.stacked\n    template: ./json_templates/mydata.jsont\n    extension: jsonl\n    linearize: True\n</code></pre></p> </li> <li> <p>Create the <code>./json_templates/mydata.jsont</code> template file (which may use Jinja) to use when rendering the data for your <code>mydata</code> destination:</p> ./json_templates/mydata.jsont<pre><code>{\n    \"column1\": \"{{column1}}\",\n    \"column2\": \"{{column2}}\",\n    \"column3\": \"{{column3}}\"\n}\n</code></pre> </li> <li> <p>Now run earthmover     <pre><code>earthmover run\n</code></pre>     and look for the output file <code>output/mydata.json</code>.</p> </li> </ol>"},{"location":"#how-it-works","title":"How it works","text":"<p><code>earthmover</code> is similar to <code>dbt</code>, though it executes data transformations locally using dataframes (rather than in a SQL engine). Like <code>dbt</code>, it creates a data dependency DAG from your <code>earthmover.yml</code> configuration and materializes output data in dependency-order.</p>"},{"location":"#read-more","title":"Read more","text":"<p>Above is a simple quick-start example. Please read the documentation for more information about <code>earthmover</code>'s many features, including:</p> <ul> <li>Resources for understanding <code>earthmover</code></li> <li>How to install <code>earthmover</code></li> <li>How to configure an <code>earthmover</code> project</li> <li>How to use <code>earthmover</code>'s commands and features</li> <li>Best practices around <code>earthmover</code> projects</li> <li>Details about the design of <code>earthmover</code></li> <li>How to contribute to <code>earthmover</code></li> </ul>"},{"location":"changelog/","title":"Changelog","text":"<p>This page tracks releases of <code>earthmover</code>, with a summary of what was changed, fixed, added in each new version.</p>"},{"location":"changelog/#2026-releases","title":"2026 releases","text":""},{"location":"changelog/#v049","title":"v0.4.9","text":"<p>(Released 2026-02-09)</p> <ul> <li>bugfix: fix links to doc site images</li> <li>bugfix: re-enable support for <code>encoding</code> field in CSV sources</li> </ul>"},{"location":"changelog/#v048","title":"v0.4.8","text":"<p>(Released 2026-01-09)</p> <ul> <li>bugfix: fix Jinja <code>import</code>s pathing under package composition</li> <li>feature: add handling for multi-line and sparse header structures</li> </ul>"},{"location":"changelog/#2025-releases","title":"2025 releases","text":""},{"location":"changelog/#v047","title":"v0.4.7","text":"<p>(Released 2025-11-13)</p> <ul> <li>bugfix: column jumbling when using <code>optional_columns</code></li> <li>bugfix: <code>earthmover init</code> failure when installed via pip</li> <li>bugfix: use <code>removeprefix()</code> not <code>lstrip()</code> when processing <code>--set</code> flags</li> <li>bugfix: prevent existing <code>value</code> column from being clobbered, if it exists</li> <li>feature: add <code>melt</code> and <code>pivot</code> dataframe operations</li> </ul>"},{"location":"changelog/#v046","title":"v0.4.6","text":"<p>(Released 2025-08-14)</p> <ul> <li>bugfix: <code>rename_columns</code> operation to an existing column name could result in two columns of the same name; now this results in an error</li> <li>feature: Add support for non-UTF8 file encodings for fixed-width inputs</li> </ul>"},{"location":"changelog/#v045","title":"v0.4.5","text":"<p>(Released 2025-07-11)</p> <ul> <li>bugfix: update MANIFEST.in to fix <code>earthmover init</code></li> <li>feature: New year, new docs!</li> <li>feature: wildcard matching for columns</li> <li>feature: multi-directional sorting</li> <li>feature: make <code>SqlSource</code>s hashable (work with state-tracking)</li> </ul>"},{"location":"changelog/#v044","title":"v0.4.4","text":"<p>(Released 2025-03-06)</p> <ul> <li>bugfix: improve exception-handling when loading a <code>SQLSource</code> using SQLAlchemy 2.x</li> </ul>"},{"location":"changelog/#v043","title":"v0.4.3","text":"<p>(Released 2025-01-23)</p> <ul> <li>feature: allow a <code>colspec_file</code> config with column info for <code>fixedwidth</code> inputs</li> <li>feature: error messages for <code>keep_columns</code> and <code>drop_columns</code> now specify the columns</li> </ul>"},{"location":"changelog/#2024-releases","title":"2024 releases","text":""},{"location":"changelog/#v042","title":"v0.4.2","text":"<p>(Released 2024-11-15)</p> <ul> <li>feature: interpolate params into destination templates</li> <li>feature: lowercase columns</li> <li>fix: optional fields recursion</li> <li>fix: <code>earthmover deps</code> fails when not all params are passed</li> <li>fix: make all pandas/dask config conditional on &gt;3.10</li> </ul>"},{"location":"changelog/#v041","title":"v0.4.1","text":"<p>(Released 2024-11-15)</p> <ul> <li>feature: allow specifying <code>colspecs</code> for fixed-width files</li> <li>feature: allow <code>config</code> params to be passable at the CLI and have a <code>parameter_default</code></li> <li>feature: refactor Source columns list logic as a select instead of a rename</li> <li>bugfix: []<code>earthmover deps</code> failed to find nested local packages](https://github.com/edanalytics/earthmover/pull/134)</li> <li>bugfix: relative paths not resolved correct when using project composition</li> <li>bugfix: <code>--results-file</code> required a directory prefix</li> <li>bugfix: some functionality was broken for Python versions &lt; 3.10</li> </ul>"},{"location":"changelog/#v040","title":"v0.4.0","text":"<p>(Released 2024-10-16)</p> <ul> <li>feature: add support for Python 3.12, with corresponding updates to core dataframe dependencies</li> <li>feature: add <code>--set</code> flag for overriding values within <code>earthmover.yml</code> from the command line</li> </ul>"},{"location":"changelog/#v038","title":"v0.3.8","text":"<p>(Released 2024-09-06)</p> <ul> <li>bugfix: Jinja in destination <code>header</code> failed if dataframe is empty</li> </ul>"},{"location":"changelog/#v037","title":"v0.3.7","text":"<p>(Released 2024-09-04)</p> <ul> <li>feature: implementing a limit_rows operation</li> <li>feature: add support for a <code>require_rows</code> boolean or non-negative int on any node</li> <li>feature: add support for Jinja in a destination node header and footer</li> <li>bugfix: union fails with duplicate columns</li> </ul>"},{"location":"changelog/#v036","title":"v0.3.6","text":"<p>(Released 2024-08-07)</p> <ul> <li>feature: add <code>json_array_agg</code> function to <code>group_by</code> operation</li> <li>feature: select all columns using \"*\" in <code>modify_columns</code> operation</li> <li>internal: set working directory to the location of the <code>earthmover.yaml</code> file</li> <li>documentation: add information on <code>earthmover init</code> and <code>earthmover clean</code> to the README</li> <li>bugfix: fix bug with <code>earthmover clean</code> that could have removed earthmover.yaml files</li> </ul>"},{"location":"changelog/#v035","title":"v0.3.5","text":"<p>(Released 2024-07-12)</p> <ul> <li>feature: add <code>earthmover init</code> command to initialize a new sample project in the expected bundle structure</li> <li>internal: expand test run to include the new <code>debug</code> and <code>flatten</code> operations, as well as a nested JSON source file</li> <li>internal: improve customization in write behavior in new file destinations</li> <li>bugfix: Fix bug when writing null values in <code>FileDestination</code></li> </ul>"},{"location":"changelog/#v034","title":"v0.3.4","text":"<p>(Released 2024-06-26)</p> <ul> <li>hotfix: Fix bug when writing out JSON in <code>FileDestination</code></li> </ul>"},{"location":"changelog/#v033","title":"v0.3.3","text":"<p>(Released 2024-06-18)</p> <ul> <li>hotfix: Resolve incompatible package dependencies</li> <li>hotfix: Fix type casting of nested JSON for destination templates</li> </ul>"},{"location":"changelog/#v032","title":"v0.3.2","text":"<p>(Released 2024-06-14)</p> <ul> <li>feature: Add <code>DebugOperation</code> for logging data head, tail, columns, or metadata midrun</li> <li>feature: Add <code>FlattenOperation</code> for splitting and exploding string columns into values</li> <li>feature: Add optional 'fill_missing_columns' field to <code>UnionOperation</code> to fill disjunct columns with nulls, instead of raising an error (default <code>False</code>)</li> <li>feature: Add <code>git_auth_timeout</code> config when entering Git credentials during package composition</li> <li>feature: Add <code>earthmover clean</code> command that removes local project artifacts</li> <li>feature: only output compiled template during <code>earthmover compile</code></li> <li>feature: Render full row into JSON lines when <code>template</code> is undefined in <code>FileDestination</code></li> <li>internal: Move <code>FileSource</code> size-checking and <code>FtpSource</code> FTP-connecting from compile to execute</li> <li>internal: Move template-file check from compile to execute in <code>FileDestination</code></li> <li>internal: Allow filepaths to be passed to an optional <code>FileSource</code>, and check for file before creating empty dataframe</li> <li>internal: Build an empty dataframe if an empty folder is passed to an optional <code>FileSource</code></li> <li>internal: fix some examples in README</li> <li>internal: remove GitPython dependency</li> <li>bugfix: fix bug in <code>FileDestination</code> where <code>linearize: False</code> resulted in BOM characters</li> <li>bugfix: fix bug where nested JSON would be loaded as a stringified Python dictionary</li> <li>bugfix: Ensure command list in help menu and log output is always consistent</li> <li>bugfix: fix bug in <code>ModifyColumnsOperation</code> where <code>__row_data__</code> was not exposed in Jinja templating</li> </ul>"},{"location":"changelog/#v031","title":"v0.3.1","text":"<p>(Released 2024-04-26)</p> <ul> <li>internal: allow any ordering of Transformations during graph-building in compile</li> <li>internal: only create a <code>/packages</code> dir when <code>earthmover deps</code> succeeds</li> </ul>"},{"location":"changelog/#v030","title":"v0.3.0","text":"<p>(Released 2024-04-17)</p> <ul> <li>feature: add project composition using <code>packages</code> keyword in template file (see README)</li> <li>feature: add installation extras for optional libraries, and improve error logging to notify which is missing</li> <li>feature: <code>GroupByWithRankOperation</code> cumulatively sums record counts by group-by columns</li> <li>feature: setting <code>log_level: DEBUG</code> in template configs or setting <code>debug: True</code> for a node displays the head of the node mid-run </li> <li>feature: add <code>optional_fields</code> key to all Sources to add optional empty columns when missing from schema</li> <li>feature: add optional <code>ignore_errors</code> and <code>exact_match</code> boolean flags to <code>DateFormatOperation</code></li> <li>internal: force-cast a dataframe to string-type before writing as a Destination</li> <li>internal: remove attempted directory-hashing when a source is a directory (i.e., Parquet)</li> <li>internal: refactor project to standardize import paths for Node and Operation</li> <li>internal: add <code>Node.full_name</code> attribute and <code>Node.set_upstream_source()</code> method</li> <li>internal: unify graph-building into compilation</li> <li>internal: refactor compilation and execution code for cleanliness</li> <li>internal: unify <code>Node.compile()</code> into initialization to ease Node development</li> <li>internal: Remove unused <code>group_by_with_count</code> and <code>group_by_with_agg</code> operations</li> </ul>"},{"location":"changelog/#v021","title":"v0.2.1","text":"<p>(Released 2024-04-08)</p> <ul> <li>feature: adding fromjson() function to Jinja</li> <li>feature: fix docs typos</li> <li>feature: <code>SortRowsOperation</code> sorts the dataset by <code>columns</code></li> </ul>"},{"location":"changelog/#2023-releases","title":"2023 releases","text":""},{"location":"changelog/#v020","title":"v0.2.0","text":"<p>(Released 2023-09-11)</p> <ul> <li>breaking change: remove <code>source</code> as Operation config and move to Transformation; this simplifies templates and reduces memory usage</li> <li>breaking change: <code>version: 2</code> required in Earthmover YAML files </li> <li>feature: <code>SnakeCaseColumnsOperation</code> converts all columns to snake_case</li> <li>feature: <code>show_progress</code> can be turned on globally in <code>config</code> or locally in any Source, Transformation, or Destination to display a progress bar</li> <li>feature: <code>repartition</code> can be turned on in any applicable <code>Node</code> to alter Dask partition-sizes post-execute</li> <li>feature: improve performance when writing Destination files</li> <li>feature: improved Earthmover YAML-parsing and config-retrieval</li> <li>internal: rename <code>YamlEnvironmentJinjaLoader</code> to <code>JinjaEnvironmentYamlLoader</code> for better transparency of use</li> <li>internal: simplify Earthmover.build_graph()</li> <li>internal: unify Jinja rendering into a single util function, instead of redeclaring across project</li> <li>internal: unify <code>Node.verify()</code> into <code>Node.execute()</code> for improved code legibility</li> <li>internal: improve attribute declarations across project</li> <li>internal: improve type-hinting and doc-strings across project</li> <li>bugfix: refactor SqlSource to be compatible with SQLAlchemy 2.x</li> </ul>"},{"location":"changelog/#v016","title":"v0.1.6","text":"<p>(Released 2023-07-11)</p> <ul> <li>bugfix: fixing a bug to create the results_file directory if needed</li> <li>bugfix: process a copy of each nodes data at each step, to avoid modifying original node data which downstreams nodes may rely on</li> </ul>"},{"location":"changelog/#v015","title":"v0.1.5","text":"<p>(Released 2023-06-13)</p> <ul> <li>bugfix: fixing a bug to skip hashing missing optional source files</li> <li>feature: adding a tmp_dir config so we can tell Dask where to store data it spills to disk</li> <li>feature: adding a <code>--results-file</code> option to produce structured run metadata</li> <li>feature: adding a skip exit code</li> </ul>"},{"location":"changelog/#v014","title":"v0.1.4","text":"<p>(Released 2023-05-12)</p> <ul> <li>bugfix: <code>config.state</code>_file was being ignored when specified</li> <li> <p>bugfix: further issues with multi-line <code>config.macros</code> - the resolution here (hopefully the last one!) is to pre-load macros (so they can be injected into run-time Jinja contexts) and then just allow the Jinja to render and macro definitions down to nothing in the config YAML... you do have to be careful with Jinja linebreak suppression, i.e.     <pre><code>config:\nmacros: &gt; # this is a macro!\n    {%- macro test() -%}\n    testing!\n    {%- endmacro -%}\nsources:\n...\n</code></pre>     could render down to     <pre><code>config:\nmacros: &gt; # this is a macro!sources:\n...\n</code></pre>     which will fail with an error about no sources defined.</p> </li> <li> <p>bugfix: charset issues when reading / writing non-UTF8 files - this should be resolved by enforcing every file read/write to specify UTF8 encoding</p> </li> </ul>"},{"location":"changelog/#v013","title":"v0.1.3","text":"<p>(Released 2023-05-05)</p> <ul> <li>feature: implement ability to call <code>{{ md5(column) }}</code> in Jinja throughout eathmover, with a framework for other Python functions to be added in the future</li> <li>bugfix: fix multi-line macros issue</li> </ul>"},{"location":"changelog/#v012","title":"v0.1.2","text":"<p>(Released 2023-05-02)</p> <ul> <li>bugfix: fix continued issues with environment variable expansion under Windows by changing from <code>os.path.expandvars()</code> to native Python <code>String.Template</code> implementation</li> <li>bugfix: change how earthmover loads <code>config.macros</code> from YAML to prevent issues with multi-line macros definitions</li> </ul>"},{"location":"changelog/#v011","title":"v0.1.1","text":"<p>(Released 2023-03-27)</p> <ul> <li>bugfix: a single quote in the config YAML could prevent environment variable expansion from working since <code>os.path.expandvars()</code> does not expand variables within single quotes in Python under Windows</li> </ul>"},{"location":"changelog/#v010","title":"v0.1.0","text":"<p>(Released 2023-03-23)</p> <ul> <li>feature: added parse-time Jinja templating to YAML configuration</li> </ul> <p> Potentially breaking change: if your config YAML contains <code>add_columns</code> or <code>modify_columns</code> operations with Jinja expressions, these will now be parsed at YAML load time. To preserve the Jinja for runtime parsing, wrap the expressions with <code>{%raw%}...{%endraw%}</code>. See YAML parsing for further information.</p> <ul> <li>feature: removed dependency on matplotlib, which is only required if your YAML specified <code>config.show_graph: True</code>... now if you try to <code>show_graph</code> without matplotlib installed, you'll get an error prompting you to install matplotlib</li> </ul>"},{"location":"changelog/#v007","title":"v0.0.7","text":"<p>(Released 2023-02-23)</p> <ul> <li>feature: added <code>str_min()</code> and <code>str_max()</code> functions for <code>group by</code> operation</li> </ul>"},{"location":"changelog/#v006","title":"v0.0.6","text":"<p>(Released 2023-02-17)</p> <ul> <li>feature: pass <code>__row_data__</code> dict into Jinja templates for easier dynamic column referencing</li> <li>bugfix: parameter / env var interpolation into YAML keys, not just values</li> <li>refactor error handling key assertion methods</li> <li>refactor YAML loader line number context handling</li> </ul>"},{"location":"changelog/#2022-releases","title":"2022 releases","text":""},{"location":"changelog/#v005","title":"v0.0.5","text":"<p>(Released 2022-12-16)</p> <ul> <li>trim nodes not connected to a destination from DAG</li> <li>ensure all source datatypes return a Dask dataframe</li> <li>update optional source functionality to require <code>columns</code> list, and pass an empty dataframe through the DAG</li> </ul>"},{"location":"changelog/#v004","title":"v0.0.4","text":"<p>(Released 2022-10-27)</p> <ul> <li>support running in Google Colab</li> </ul>"},{"location":"changelog/#v003","title":"v0.0.3","text":"<p>(Released 2022-10-27)</p> <ul> <li>support for Python 3.7</li> </ul>"},{"location":"changelog/#v002","title":"v0.0.2","text":"<p>(Released 2022-09-22)</p> <ul> <li>initial release</li> </ul>"},{"location":"configuration/","title":"Project configuration","text":""},{"location":"configuration/#project-structure","title":"Project structure","text":"<p>An <code>earthmover</code> project consists of</p> <ul> <li>source data to be transformed, such as CSV or TSV files, or a relational database table</li> <li>an <code>earthmover.yml</code> YAML configuration file that defines<ul> <li>your project <code>config</code></li> <li>(optional) <code>definitions</code> of YAML anchors to be used elsewhere</li> <li>(optional) <code>packages</code> to include in your project</li> <li>data <code>sources</code> to transform</li> <li>data <code>transformations</code> to execute</li> <li>data <code>destinations</code> to materialize</li> </ul> </li> <li>(optional) Jinja templates to be rendered for each row of the final <code>destination</code></li> </ul> Tips <ul> <li>To quickly see <code>earthmover</code> in action, run <code>earthmover init</code> to initialize a simple starter project.</li> <li>Example <code>earthmover</code> projects can be found in the example_projects/ folder.</li> <li>See the recommended best practices for building an <code>earthmover</code> project</li> </ul> <p>Within an earthmover project folder, you can simply <pre><code>earthmover run\n</code></pre> to run the data transformations.</p>"},{"location":"configuration/#yaml-configuration","title":"YAML configuration","text":"<p>All the instructions for <code>earthmover</code> \u2014 where to find the source data, what transformations to apply to it, and how and where to save the output \u2014 are specified in a YAML configuration file, typically named <code>earthmover.yml</code>.</p> <p>Note that <code>earthmover.yml</code> may also contain Jinja and/or parameters which are rendered in an initial compilation step before execution.</p>"},{"location":"configuration/#version","title":"<code>version</code>","text":"<p>A <code>version</code> label is required in <code>earthmover.yml</code> for compatibility reasons. The current <code>version</code> is <code>2</code>.</p> earthmover.yml<pre><code>version: 2\n</code></pre>"},{"location":"configuration/#config","title":"<code>config</code>","text":"<p>The <code>config</code> section specifies various options for the operation of this tool.</p> <p>A sample <code>config</code> section is shown here; the options are explained below.</p> earthmover.yml<pre><code>config:\n  output_dir: ./\n  state_file: ~/.earthmover.csv\n  log_level: INFO\n  tmp_dir: /tmp\n  show_stacktrace: True\n  show_graph: True\n  macros: &gt;\n    {% macro example_macro(value) -%}\n        prefixed-int-{{value|int}}\n    {%- endmacro %}\n  parameter_defaults:\n    SOURCE_DIR: ./sources/\n  show_progress: True\n  git_auth_timeout: 120\n</code></pre> Required? Key Type Description Default value (optional) <code>output_dir</code> <code>string</code> The folder where <code>destinations</code> will be materialized. <code>./</code> (current directory) (optional) <code>state_file</code> <code>string</code> The file where tool state is maintained. <code>~/.earthmover.csv</code> on *nix systems, <code>C:/Users/USER/.earthmover.csv</code> on Windows systems (optional) <code>log_level</code> <code>string</code> The console output verbosity. Options include: <ul><li><code>ERROR</code>: only output important errors like invalid YAML configuration, missing required sources, invalid DAG node references, etc.</li><li><code>WARNING</code>: output errors and warnings, like when tool state becomes large</li><li><code>INFO</code>: all errors and warnings plus basic information about what earthmover is doing: start and stop, how many rows were removed by a <code>distinct_rows</code> or <code>filter_rows</code> operation, etc. (the default <code>log_level</code>)</li><li><code>DEBUG</code>: all output above, plus verbose details about each transformation step, timing, memory usage, and more (recommended for debugging transformations.)</li></ul> <code>INFO</code> (optional) <code>tmp_dir</code> <code>string</code> The folder to use when dask must spill data to disk. <code>/tmp</code> (optional) <code>show_graph</code> <code>boolean</code> Whether  or not to create <code>./graph.png</code> and <code>./graph.svg</code> showing the data dependency graph. (Requires PyGraphViz to be installed.) <code>False</code> (optional) <code>show_stacktrace</code> <code>boolean</code> Whether to show a stacktrace for runtime errors. <code>False</code> (optional) <code>macros</code> <code>string</code> Jinja macros which will be available within any template throughout the project. (This can slow performance.) (none) (optional) <code>parameter_defaults</code> <code>dict</code> Default values to be used if the user fails to specify a parameter. (none) (optional) <code>show_progress</code> <code>boolean</code> Whether to show a progress bar for each Dask transformation <code>False</code> (optional) <code>git_auth_timeout</code> <code>integer</code> Number of seconds to wait for the user to enter Git credentials if needed during package installation (see project composition) <code>60</code>"},{"location":"configuration/#definitions","title":"<code>definitions</code>","text":"<p>The (optional) <code>definitions</code> section can be used to define YAML elements which are reused throughout the rest of the configuration. <code>earthmover</code> does nothing special with this section, it's just interpreted by the YAML parser. However, this can be a very useful way to keep your YAML configuration DRY \u2013 rather than redefine the same values, Jinja phrases, etc. throughout your config, define them once in this section and refer to them later using YAML anchors, aliases, and overrides.</p> <p>An example <code>definitions</code> section, and how it can be used later on, are shown below:</p> earthmover.yml<pre><code>definitions:\n  operations:\n    - &amp;student_join_op\n      operation: join\n      join_type: left\n      left_key: student_id\n      right_key: student_id\n  ...\n  date_to_year_jinja: &amp;date_to_year \"{%raw%}{{ val[-4:] }}{%endraw%}\"\n...\n\ntransformations:\n  roster:\n    operations:\n      - &lt;&lt;: *student_join_op\n        sources:\n        - $sources.roster\n        - $sources.students\n  enrollment:\n    operations:\n      - &lt;&lt;: *student_join_op\n        sources:\n        - $sources.enrollment\n        - $sources.students\n  ...\n  academic_terms:\n    operations:\n      - operation: duplicate_columns\n        source: $sources.academic_terms\n        columns:\n          start_date: school_year\n      - operation: modify_columns\n        columns:\n          school_year: *date_to_year\n</code></pre>"},{"location":"configuration/#packages","title":"<code>packages</code>","text":"<p>The (optional) <code>packages</code> section can be used to specify packages \u2013 other earthmover projects from a local directory or GitHub \u2013 to import and build upon exisiting code. See Project Composition for details and considerations.</p> <p>A sample <code>package</code>s section is shown here; the options are explained below.</p> <p>earthmover.yml<pre><code>packages:\n  year_end_assessment:\n    git: https://github.com/edanalytics/earthmover_edfi_bundles.git\n    subdirectory: assessments/assessment_name\n  student_id_macros:\n    local: path/to/student_id_macros\n</code></pre> Each package must have a name (which will be used to name the folder where it is installed in <code>./packages/</code>) such as <code>year_end_assessment</code> or <code>student_id_macros</code> in the above example. Two sources of packages are currently supported:</p> <ul> <li> <p>GitHub packages: Specify the URL of the repository containing the package. If the package YAML configuration is not in the top level of the repository, include the path to the folder with the the optional subdirectory.</p> <p>Tip</p> <p><code>earthmover</code> uses the system's <code>git</code> client to clone packages from GitHub. To access non-public packages, the <code>git</code> client must have authentication configured separately.</p> </li> <li> <p>Local packages: Specify the path to the folder containing the package YAML configuration. Paths may be absolute or relative paths to the location of the <code>earthmover</code> YAML configuration file.</p> </li> </ul>"},{"location":"configuration/#sources","title":"<code>sources</code>","text":"<p>The <code>sources</code> section specifies source data <code>earthmover</code> will transform.</p> <p>A sample <code>sources</code> section is shown here; the options are explained below.</p> earthmover.yml<pre><code>sources:\n  districts:\n    connection: \"ftp://user:pass@host:port/path/to/districts.csv\"\n  tx_schools:\n    connection: \"postgresql://user:pass@host/database\"\n    query: &gt;\n      select school_id, school_name, school_website\n      from schema.schools\n      where school_address_state='TX'\n  courses:\n    file: ./data/Courses.csv\n    header_rows: 1\n    columns:\n      - school_id\n      - school_year\n      - course_code\n      - course_title\n      - subject_id\n  more_schools:\n    file: ./data/Schools.csv\n    header_rows: 1\n    columns:\n      - school_id\n      - school_name\n      - address\n      - phone_number\n    expect:\n      - low_grade != ''\n      - high_grade != ''\n      - low_grade|int &lt;= high_grade|int\n</code></pre> <p>Each source must have a name (which is how it is referenced by transformations and destinations) such as <code>districts</code>, <code>courses</code>, <code>tx_schools</code>, or <code>more_schools</code> in the above example. Currently-supported source types include:</p> source type format file type notes file row-based <code>.csv</code> Specify the number of <code>header_rows</code>, and (if <code>header_rows</code> &gt; 0, optionally) overwrite the <code>column</code> names. Optionally specify an <code>encoding</code> to use when reading the file (the default is UTF8). See also multi-line &amp; sparse headers. <code>.tsv</code> Specify the number of <code>header_rows</code>, and (if <code>header_rows</code> &gt; 0, optionally) overwrite the <code>column</code> names. Optionally specify an <code>encoding</code> to use when reading the file (the default is UTF8). See also multi-line &amp; sparse headers. <code>.txt</code> A fixed-width text file; see the documentation below for configuration details. column-based <code>.parquet</code>, <code>.feather</code>, <code>.orc</code> These require the <code>pyarrow</code> library, which can be installed with <code>pip install pyarrow</code> or similar structured <code>.json</code> Optionally specify a <code>object_type</code> (<code>frame</code> or <code>series</code>) and <code>orientation</code> (see these docs) to interpret different JSON structures. <code>.jsonl</code> or <code>.ndjson</code> Files with a flat JSON structure per line. <code>.xml</code> Optionally specify an <code>xpath</code> to select a set of nodes deeper in the XML. <code>.html</code> Optionally specify a regex to <code>match</code> for selecting one of many tables in the HTML. This can be used to extract tables from a live web page. Excel <code>.xls</code>, <code>.xlsx</code>, <code>.xlsm</code>, <code>.xlsb</code>, <code>.odf</code>, <code>.ods</code> and <code>.odt</code> Optionally specify the <code>sheet</code> name (as a string) or index (as an integer) to load. See also multi-line headers. other <code>.pkl</code> or <code>.pickle</code> A pickled Python object (typically a Pandas dataframe) <code>.sas7bdat</code> A SAS data file <code>.sav</code> A SPSS data file <code>.dta</code> A Stata data file Database various - Database sources are supported via SQLAlchemy. They must specify a database <code>connection</code> string and SQL <code>query</code> to run. FTP various - FTP file sources are supported via ftplib. They must specify an FTP <code>connection</code> string with the path to the file. <p>File <code>type</code> is inferred from the file extension, however you may manually specify <code>type:</code> (<code>csv</code>, <code>tsv</code>, <code>fixedwidth</code>, <code>parquet</code>, <code>feather</code>, <code>orc</code>, <code>json</code>, <code>jsonl</code>, <code>xml</code>, <code>html</code>, <code>excel</code>, <code>pickle</code>, <code>sas</code>, <code>spss</code>, or <code>stata</code>) to force <code>earthmover</code> to treat a file with an arbitrary extension as a certain type. Remote file paths (<code>https://somesite.com/path/to/file.csv</code>) generally work.</p>"},{"location":"configuration/#multi-line-sparse-headers","title":"Multi-line &amp; sparse headers","text":"<p><code>csv</code>, <code>tsv</code>, and <code>excel</code> files may have multi-line headers.</p> Multi-line header example <p><pre><code>,,X,X,Y,Y\na,b,a,b,a,b\n1,2,3,4,5,6\n7,8,9,10,11,12\n</code></pre> (Both header lines are needed to disambiguate between columns <code>X.a</code> and <code>Y.a</code>.)</p> <p><code>earthmover</code> supports multi-line headers via a list of (0-based) <code>header_rows</code> indices. (Omitted row indices are skipped.) The final column names will be formed by joining column names from each level with two underscores (<code>__</code>).</p> <p>Furthermore, some <code>csv</code> and <code>tsv</code> files may have sparse multi-line headers.</p> Sparse multi-line header example <p><pre><code>,,X,,Y,\na,b,a,b,a,b\n1,2,3,4,5,6\n7,8,9,10,11,12\n</code></pre> (In the first header row, <code>X</code> and <code>Y</code> can be filled to the right to produce the first example.)</p> <p><code>earthmover</code> additionally supports filling in sparse headers with <code>fill_sparse_headers: True</code>, which fills non-empty column names to the right \u2014 up to the next non-empty column name.</p> <p>Configuration example</p> <p>Puting these together, an <code>earthmover.yml</code> such as <pre><code>sources:\n  mydata:\n    file: ./example.csv\n    header_rows: [0, 1]\n    fill_sparse_headers: True\n</code></pre> with a file like <pre><code>,,X,,Y,\na,b,a,b,a,b\n1,2,3,4,5,6\n7,8,9,10,11,12\n</code></pre> will produce a dataframe with column names like</p> a b X__a X__B Y__a Y__b 1 2 3 4 5 6 7 8 9 10 11 12"},{"location":"configuration/#fixed-width-config","title":"Fixed-width config","text":"<p>Using a fixed-width file (FWF) as a source requires additional metadata, configuring how <code>earthmover</code> should slice each row into its constituent columns. Two ways to provide this metadata are supported:</p> Provide a <code>colspec_file</code> <p>Example configuration for a <code>fixedwidth</code> source with a <code>colspec_file</code>:</p> <pre><code>sources:\n  input:\n    file: ./data/input.txt\n    colspec_file: ./seed/colspecs.csv # required\n    colspec_headers:\n      name: field_name                # required\n      start: start_index              # required if `width` is not provided\n      end: end_index                  # required if `width` is not provided\n      width: field_length             # required if `start` or `end` is not provided\n    type: fixedwidth                  # required if `file` does not end with '.txt'\n    header_rows: 0\n</code></pre> <p>Notes:</p> <ul> <li>(required) <code>colspec_file</code>: path to the CSV containing <code>colspec</code> details</li> <li>(required) <code>colspec_headers</code>: mapping between the <code>colspec_file</code>'s column names and fields required by <code>earthmover</code>. (Columns may have any name and position.)<ul> <li>Only <code>name</code> is always required: <code>colspec_file</code> must contain a column that assigns a name to each field in the FWF</li> <li>Either <code>width</code> or both <code>start</code> and <code>end</code> are required<ul> <li>If <code>width</code> is provided, <code>colspec_file</code> should include a column of integer values indicating the number of characters in each field in the FWF</li> <li>If <code>start</code> and <code>end</code> are provided, <code>colspec_file</code> should include two columns of integer values giving extents of the FWF's fields as half-open intervals (i.e., [from, to[ ) </li> </ul> </li> </ul> </li> <li>(optional) <code>type</code>: optional if source file has <code>.txt</code> extension; otherwise, specify <code>type: fixedwidth</code> (there is no standard file extension for FWFs)</li> <li>(optional) <code>header_rows</code>: usually 0 for FWFs; <code>earthmover</code> attempts to infer if not specified</li> </ul> <p>Formatting a <code>colspec_file</code></p> <p>A <code>colspec_file</code> must include a column with field names, and</p> <ol> <li>either a column with field widths</li> <li>or two columns with start and end positions</li> </ol> <p>Example of (1):</p> <p><pre><code>name,width\ndate,8\nid,16\nscore_1,2\nscore_2,2\n</code></pre> with <code>earthmover.yaml</code> like:</p> <pre><code>colspec_headers:\n  name: name\n  width: width\n</code></pre> <p>Example of (2):</p> <p><pre><code>start_idx, end_idx, other_data, full_field_name, other_data_2\n0, 8, abc, date, def\n8, 24, abc, id, def\n24, 26, abc, score_1, def\n26, 28, abc, score_2, def\n</code></pre> with <code>earthmover.yaml</code> like:</p> <pre><code>colspec_headers:\n  name: full_field_name\n  start: start_idx\n  end: end_idx\n</code></pre> Provide <code>colspecs</code> and <code>columns</code> <p>Example configuration for a <code>fixedwidth</code> source with a <code>colspecs</code> and <code>columns</code>:</p> <pre><code>sources:\n  input:\n    file: ./data/input.txt\n    type: fixedwidth        # required if `file` does not end with '.txt'\n    header_rows: 0\n    colspecs:               # required\n      - [0, 8]\n      - [8, 24]\n      - [24, 26]\n      - [26, 28]\n    columns:                # required\n      - date\n      - id\n      - score_1\n      - score_2\n</code></pre> <p>Notes:</p> <ul> <li>(required) <code>colspecs</code>: list of start and end indices giving extents of the FWF's fields as half-open intervals (i.e., [from, to[ ) </li> <li>(required) <code>columns</code>: list of column names corresponding to the indices in <code>colspecs</code></li> </ul>"},{"location":"configuration/#source-examples","title":"<code>source</code> examples","text":"<code>.csv</code><code>.tsv</code><code>.txt</code> (fixed-width file)<code>.parquet</code><code>.json</code><code>.jsonl</code><code>.xml</code>DatabaseFTP file<code>.csv</code> (multi-line &amp; sparse header) <pre><code>sources:\n  mydata:\n    file: ./data/mydata.csv\n    header_rows: 1\n</code></pre> <p>(for an input file like...)</p> mydata.csv<pre><code>id,year,code\n1234,2005,54321\n1235,2006,54322\n...\n</code></pre> <pre><code>sources:\n  mydata:\n    file: ./data/mydata.tsv\n    header_rows: 1\n</code></pre> <p>(for an input file like...)</p> mydata.tsv<pre><code>id  year    code\n1234    2005   54321\n1235    2006   54322\n...\n</code></pre> <pre><code>sources:\n  mydata:\n    file: ./data/mydata.txt\n    colspecs:\n      - [0, 4]   # id\n      - [6, 9]   # year\n      - [10, 20] # code\n      - ...\n    columns:\n      - id\n      - year\n      - code\n      - ...\n    # or\n    colspec_file: ./mydata_colspec.csv\n    # where `mydata_colspec.csv` is a file with columns `col_name`, `start_pos`, and `end_pos`\n</code></pre> <p>(for an input file like...)</p> mydata.txt<pre><code>1234 2005    54321\n1235 2006    54322\n...\n</code></pre> <pre><code>sources:\n  mydata:\n    file: ./data/mydata.parquet\n    columns:\n    - school_id\n    - school_year\n    - course_code\n    - ...\n</code></pre> <pre><code>sources:\n  mydata:\n    file: ./data/mydata.json\n    orientation: records\n</code></pre> <p>(for an input file like...)</p> mydata.json<pre><code>[\n    {\n        \"column1\": \"value1\",\n        \"column2\": \"value2\",\n        ...\n    },\n    {\n        \"column1\": \"value3\",\n        \"column2\": \"value4\",\n        ...\n    },\n    ...\n]\n</code></pre> <pre><code>sources:\n  mydata:\n    file: ./data/mydata.jsonl\n</code></pre> <p>(for an input file like...)</p> mydata.jsonl<pre><code>{ \"column1\": \"value1\", \"column2\": \"value2\", ...}\n{ \"column1\": \"value3\", \"column2\": \"value4\", ...}\n...\n</code></pre> <pre><code>sources:\n  mydata:\n    file: ./data/mydata.xml\n    xpath: //SomeElement\n</code></pre> <p>(for an input file like...)</p> mydata.xml<pre><code>&lt;SomeElement&gt;\n    &lt;Attribute1&gt;Value1&lt;/Attribute1&gt;\n    &lt;Attribute2&gt;Value2&lt;/Attribute2&gt;\n    ...\n&lt;/SomeElement&gt;\n&lt;SomeElement&gt;\n    &lt;Attribute1&gt;Value3&lt;/Attribute1&gt;\n    &lt;Attribute2&gt;Value4&lt;/Attribute2&gt;\n    ...\n&lt;/SomeElement&gt;\n...\n</code></pre> <pre><code>sources:\n  mydata:\n    connection: \"postgresql://user:pass@host/mydatabase\"\n    query: &gt;\n      select id, year, code, ...\n      from myschema.mytable\n      where some_column='some_value'\n</code></pre> <pre><code>sources:\n  mydata:\n    connection: \"ftp://user:pass@host:port/path/to/mydata.csv\"\n</code></pre> <pre><code>sources:\n  mydata:\n    file: ./sources/mydata.csv\n    header_rows: [0,2,3]\n    fill_sparse_headers: True\n</code></pre> <p>(for an input file like...)</p> mydata.csv<pre><code>I,,,,II,,,,III,,,,IV,,,\n--- this line is ignored ---\n1,,2,,3,,4,,5,,6,,7,,8,\na,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p\n00,01,02,03,04,05,06,07,08,09,0A,0B,0C,0D,0E,0F\n10,11,12,13,14,15,16,17,18,19,1A,1B,1C,1D,1E,1F\n...\n</code></pre>"},{"location":"configuration/#expectations","title":"<code>expect</code>ations","text":"<p>For any source, optionally specify conditions you <code>expect</code> data to meet which, if not true for any row, will cause the run to fail with an error. (This can be useful for detecting and rejecting NULL or missing values before processing the data.) The format must be a Jinja expression that returns a boolean value. This is enables casting values (which are all treated as strings) to numeric formats like int and float for numeric comparisons.</p> <pre><code>sources:\n  school_directory_grades_only:\n    file: ./sources/school_directory.csv\n    header_rows: 1\n    expect:\n      - \"low_grade != ''\"\n      - \"high_grade != ''\"\n    # ^ require a `low_grade` and a `high_grade`\n</code></pre>"},{"location":"configuration/#credentials","title":"credentials","text":"<p>The Database and FTP examples above show <code>user:pass</code> in the connection string, but if you are version-controlling your <code>earthmover</code> project, you must avoid publishing such credentials. Typically this is done via parameters, which may be referenced throughout <code>earthmover.yml</code> (not just in the <code>sources</code> section), and are parsed at run-time. For example:</p> <p><pre><code>sources:\n  mydata:\n    connection: \"postgresql://${DB_USERNAME}:${DB_PASSWORD}@host/mydatabase\"\n    query: &gt;\n      select id, year, code, ...\n      from myschema.mytable\n      where some_column='some_value'\n</code></pre> and then <pre><code>export DB_USERNAME=myuser\nexport DB_PASSWORD=mypa$$w0rd\nearthmover run\n# or\nearthmover run -p '{\"DB_USERNAME\":\"myuser\", \"DB_PASSWORD\":\"mypa$$w0rd\"}'\n</code></pre></p>"},{"location":"configuration/#transformations","title":"<code>transformations</code>","text":"<p>The <code>transformations</code> section specifies how source data is transformed by <code>earthmover</code>.</p> <p>A sample <code>transformations</code> section is shown here; the options are explained below.</p> earthmover.yml<pre><code>transformations:\n  courses:\n    source: $sources.courses\n    operations:\n      - operation: map_values\n        column: subject_id\n        mapping:\n          01: 1 (Mathematics)\n          02: 2 (Literature)\n          03: 3 (History)\n          04: 4 (Language)\n          05: 5 (Computer and Information Systems)\n      - operation: join\n        sources:\n          - $sources.schools\n        join_type: inner\n        left_key: school_id\n        right_key: school_id\n      - operation: drop_columns\n        columns:\n          - address\n          - phone_number\n</code></pre> <p>The above example shows a transformation of the courses source, which consists of an ordered list of operations. A transformation defines a source to which a series of operations are applied. This source may be an original <code>$source</code> or another <code>$transformation</code>. Transformation operations each require further specification depending on their type; the operations are listed and documented below.</p>"},{"location":"configuration/#frame-operations","title":"Frame operations","text":"union <p>Concatenates or \"stacks\" the transformation source with one or more sources of the same shape.</p> <pre><code>    - operation: union\n        sources:\n        - $sources.courses_list_1\n        - $sources.courses_list_2\n        - $sources.courses_list_3\n        fill_missing_columns: False\n</code></pre> join <p>Joins the transformation source with one or more sources.</p> <pre><code>    - operation: join\n        sources:\n        - $sources.schools\n        join_type: inner | left | right\n        left_key: school_id\n        right_key: school_id\n        # or:\n        left_keys:\n        - school_id\n        - school_year\n        right_keys:\n        - school_id\n        - school_year\n        # optionally specify columns to (only) keep from the left and/or right sources:\n        left_keep_columns:\n        - left_col_1\n        - left_col_2\n        right_keep_columns:\n        - right_col_1\n        - right_col_2\n        # or columns to discard from the left and/or right sources:\n        left_drop_columns:\n        - left_col_1\n        - left_col_2\n        right_drop_columns:\n        - right_col_1\n        - right_col_2\n        # (if neither ..._keep nor ..._drop are specified, all columns are retained)\n</code></pre> <p>Joining can lead to a wide result; the <code>..._keep_columns</code> and <code>..._drop_columns</code> options enable narrowing it.</p> <p>Besides the join column(s), if a column <code>my_column</code> with the same name exists in both tables and is not dropped, it will be renamed <code>my_column_x</code> and <code>my_column_y</code>, from the left and right respectively, in the result.</p> melt <p>Unpivots the source from wide to long format, where the names of columns become values in a generic \"variable\" column, and the values of those columns shift to a generic \"values\" column</p> <pre><code>    - operation: melt\n        sources:\n        - $sources.score_col_for_each_subject\n        # Column(s) from the wide dataset to keep as row identifiers in the long dataset\n        id_vars: student_id\n        # Column(s) from the wide dataset to translate into values of the \"variable\" columns. If omitted, melt all columns not in id_vars. \n        value_vars: [math_score, reading_score, science_score, writing_score]\n        # Optional name of the column containing value_vars. default to \"melt_variable\"\n        var_name: subject\n        # Optional name of the column containing the values of the value_vars columns. Default to \"melt_value\"\n        value_name: score\n</code></pre> pivot <p>Opposite of <code>melt</code> - pivots a source from long to wide format, where the unique values of old columns become the column names of new ones</p> <pre><code>    - operation: pivot\n        sources:\n        - $sources.melted_scores\n        # Column in long dataset whose unique values are names of columns in the wide dataset - to pivot by multiple columns, use sequential pivots\n        cols_by: subject\n        # Column(s) in long dataset that define the rows in the wide dataset\n        rows_by: student_id\n        # Column(s) in long dataset used to populate the wide dataset's columns\n        values: score\n</code></pre>"},{"location":"configuration/#column-operations","title":"Column operations","text":"Tip: use Jinja! <p>The <code>add_columns</code> and <code>modify_columns</code> operations (documented below) support the use of Jinja templating to add/modify values dynamically. In the column definitions for such operations:</p> <ul> <li><code>{{value}}</code> refers to this column's value</li> <li><code>{{AnotherColumn}}</code> refers to another column's value</li> <li>Any Jinja filters and math operations should work</li> <li>Reference the current row number with <code>{{__row_number__}}</code></li> <li>Reference a dictionary containing the row data with <code>{{__row_data__['column_name']}}</code></li> </ul> <p>Jinja expressions must be wrapped in <code>{%raw%}...{%endraw%}</code> to avoid them being parsed by the initial parsing step.</p> Tip: wildcard matching for column names <p>The <code>modify_columns</code>, <code>drop_columns</code>, <code>keep_columns</code>, <code>combine_columns</code>, <code>map_values</code>, and <code>date_format</code> operations (documented below) support the use of wildcard matching (via fnmatch) so apply a transformation to multiple columns. Example:</p> <pre><code>- operation: drop_columns\n  columns:\n    - \"*_suffix\"             # matches `my_suffix` and `your_suffix`\n    - prefix_*             # matches `prefix_1` and `prefix_2`\n    - \"*_sandwich_*\"         # matches `my_sandwich_1` and `your_sandwich_2`\n    - single_character_?   # matches `single_character_A`, `single_character_B`, etc.\n    - number_[0123456789]  # matches `number_1`, `number_2`, etc.\n</code></pre> <p>Note that this feature means that if your data frame's columns legitimately contain the special characters <code>*</code>, <code>?</code>, <code>[</code>, or <code>]</code>, you must first use <code>rename_columns</code> to remove those characters before using an operation that supports wildcard matching to avoid unexpected results.</p> <p>Note also that column specifications cannot begin with <code>*</code> or the YAML parser would interpret them as an anchor reference. Hence the examples above are quoted (<code>\"*_...\"</code>), which is the proper way to use wildcard prefixes while avoiding YAML parse errors.</p> add_columns <p>Adds columns with specified values.</p> <pre><code>    - operation: add_columns\n        columns:\n        new_column_1: value_1\n        new_column_2: \"{%raw%}{% if True %}Jinja works here{% endif %}{%endraw%}\"\n        new_column_3: \"{%raw%}Reference values from {{AnotherColumn}} in this new column{%endraw%}\"\n        new_column_4: \"{%raw%}{% if col1&gt;col2 %}{{col1|float + col2|float}}{% else %}{{col1|float - col2|float}}{% endif %}{%endraw%}\"\n</code></pre> rename_columns <p>Renames columns.</p> <pre><code>    - operation: rename_columns\n        columns:\n        old_column_1: new_column_1\n        old_column_2: new_column_2\n        old_column_3: new_column_3\n</code></pre> duplicate_columns <p>Duplicates columns (and all their values).</p> <pre><code>    - operation: duplicate_columns\n        columns:\n        existing_column1: new_copy_of_column1\n        existing_column2: new_copy_of_column2\n</code></pre> drop_columns <p>Removes the specified columns.</p> <pre><code>    - operation: drop_columns\n        columns:\n        - column_to_drop_1\n        - column_to_drop_2\n</code></pre> keep_columns <p>Keeps only the specified columns, discards the rest.</p> <pre><code>    - operation: keep_columns\n        columns:\n        - column_to_keep_1\n        - column_to_keep_2\n</code></pre> combine_columns <p>Combines the values of the specified columns, delimited by a separator, into a new column.</p> <pre><code>    - operation: combine_columns\n        columns:\n        - column_1\n        - column_2\n        new_column: new_column_name\n        separator: \"_\"\n</code></pre> modify_columns <p>Modify the values in the specified columns.</p> <pre><code>    - operation: modify_columns\n        columns:\n        state_abbr: \"{%raw%}XXX{{value|reverse}}XXX{%endraw%}\"\n        school_year: \"{%raw%}20{{value[-2:]}}{%endraw%}\"\n        zipcode: \"{%raw%}{{ value|int ** 2 }}{%endraw%}\"\n        \"*\": \"{%raw%}{{value|trim}}{%endraw%}\" # Edit all values in dataframe\n</code></pre> map_values <p>Map the values of a column.</p> <pre><code>    - operation: map_values\n        column: column_name\n        # or, to map multiple columns simultaneously\n        columns:\n        - col_1\n        - col_2\n        mapping:\n        old_value_1: new_value_1\n        old_value_2: new_value_2\n        # or a CSV/TSV with two columns (from, to) and header row\n        # paths may be absolute or relative paths to the location of the `earthmover` YAML configuration   file\n        map_file: path/to/mapping.csv\n</code></pre> date_format <p>Change the format of a date column.</p> <pre><code>    - operation: date_format\n        column: date_of_birth\n        # or\n        columns:\n        - date_column_1\n        - date_column_2\n        - date_column_3\n        from_format: \"%b %d %Y %H:%M%p\"\n        to_format: \"%Y-%m-%d\"\n        ignore_errors: False  # Default False\n        exact_match: False    # Default False\n</code></pre> <p>The <code>from_format</code> and <code>to_format</code> must follow Python's strftime() and strptime() formats.</p> <p>When <code>ignore_errors</code> is <code>True</code>, empty strings will be replaced with Pandas NaT (not-a-time) datatypes. This ensures column-consistency and prevents a mix of empty strings and timestamps.</p> <p>When <code>exact_match</code> is <code>True</code>, the operation will only run successfully if the <code>from_format</code> input exactly matches the format of the date column. When <code>False</code>, the operation allows the format to partially-match the target string.</p> snake_case_columns <p>Force the names of all columns to snake_case.</p> <pre><code>    - operation: snake_case_columns\n</code></pre>"},{"location":"configuration/#row-operations","title":"Row operations","text":"distinct_rows <p>Removes duplicate rows.</p> <pre><code>    - operation: distinct_rows\n        columns:\n        - distinctness_column_1\n        - distinctness_column_2\n</code></pre> <p>Optionally specify the <code>columns</code> to use for uniqueness, otherwise all columns are used. If duplicate rows are found, only the first is kept.</p> filter_rows <p>Filter (include or exclude) rows matching a query.</p> <pre><code>    - operation: filter_rows\n        query: school_year &lt; 2020\n        behavior: exclude # or `include`\n</code></pre> <p>The query format is anything supported by Pandas.DataFrame.query. Specifying behavior as <code>exclude</code> wraps the Pandas <code>query()</code> with <code>not()</code>.</p> sort_rows <p>Sort rows by one or more columns.</p> <p>Simple sorting <pre><code>    - operation: sort_rows\n        columns:\n        - sort_column_1\n        descending: False\n</code></pre></p> <p>By default, rows are sorted ascendingly. Set <code>descending: True</code> to reverse this order.</p> <p>Multidirectional sorting (new syntax) Specify the sort direction for each column using + (ascending) or - (descending)</p> <p><pre><code>    - operation: sort_rows\n        columns:\n        - +sort_column_1 # \"+\" is optional; ascending by default\n        - -sort_column_2 # \"-\" indicates descending;\n</code></pre> Or use a compact list format:</p> <pre><code>    - operation: sort_rows\n        columns: [ sort_column_1, -sort_column_2 ]\n</code></pre> <p>Tip</p> <p><code>earthmover</code> cannot distinguish between a leading <code>+</code> that is part of the column name vs. a leading <code>+</code> that denotes \"sort ascendingly;\" the first leading <code>+</code> will always be removed. Consider prefixing the column explicitly with the sorting direction (e.g. <code>++sort_column_1</code>) or renaming the column using <code>rename_columns</code>.</p> limit_rows <p>Limit the number of rows in the dataframe.</p> <pre><code>    - operation: limit_rows\n        count: 5 # required, no default\n        offset: 10 # optional, default 0\n</code></pre> <p>(If fewer than count rows in the dataframe, they will all be returned.)</p> flatten <p>Split values in a column and create a copy of the row for each value.</p> <pre><code>    - operation: flatten\n        flatten_column: my_column\n        left_wrapper: '[\"' # characters to trim from the left of values in `flatten_column`\n        right_wrapper: '\"]' # characters to trim from the right of values in `flatten_column`\n        separator: \",\"  # the string by which to split values in `flatten_column`\n        value_column: my_value # name of the new column to create with flattened values\n        trim_whitespace: \" \\t\\r\\n\\\"\" # characters to trim from `value_column` _after_ flattening\n</code></pre> <p>The defaults above are designed to allow flattening JSON arrays (in a string) with simply</p> <pre><code>    - operation: flatten\n        flatten_column: my_column\n        value_column: my_value\n</code></pre> <p>Note that for empty string values or empty arrays, a row will still be preserved. These can be removed in a second step with a <code>filter_rows</code> operation. Example:</p> <pre><code># Given a dataframe like this:\n#   foo     bar    to_flatten\n#   ---     ---    ----------\n#   foo1    bar1   \"[\\\"test1\\\",\\\"test2\\\",\\\"test3\\\"]\"\n#   foo2    bar2   \"\"\n#   foo3    bar3   \"[]\"\n#   foo4    bar4   \"[\\\"test4\\\",\\\"test5\\\",\\\"test6\\\"]\"\n# \n# a flatten operation like this:\n    - operation: flatten\n        flatten_column: to_flatten\n        value_column: my_value\n# will produce a dataframe like this:\n#   foo     bar    my_value\n#   ---     ---    --------\n#   foo1    bar1   test1\n#   foo1    bar1   test2\n#   foo1    bar1   test3\n#   foo2    bar2   \"\"\n#   foo3    bar3   \"\"\n#   foo4    bar4   test4\n#   foo4    bar4   test5\n#   foo4    bar4   test6\n#\n# and you can remove the blank rows if needed with a further operation:\n    - operation: filter_rows\n        query: my_value == ''\n        behavior: exclude\n</code></pre>"},{"location":"configuration/#group-operations","title":"Group operations","text":"group_by <p>Reduce the number of rows by grouping, and add columns with values calculated over each group.</p> <pre><code>    - operation: group_by\n        group_by_columns:\n        - student_id\n        create_columns:\n        num_scores: count()\n        min_score: min(item_score)\n        max_score: max(item_score)\n        avg_score: mean(item_score)\n        item_scores: agg(item_score,;)\n</code></pre> <p>Valid aggregation functions are</p> <ul> <li><code>count()</code> or <code>size()</code> - the number of rows in each group</li> <li><code>min(column)</code> - the minimum (numeric) value in <code>column</code> for each group</li> <li><code>str_min(column)</code> - the minimum (string) value in <code>column</code> for each group</li> <li><code>max(column)</code> - the maximum (numeric) value in <code>column</code> for each group</li> <li><code>str_max(column)</code> - the maximum (string) value in <code>column</code> for each group</li> <li><code>sum(column)</code> - the sum of (numeric) values in <code>column</code> for each group</li> <li><code>mean(column)</code> or <code>avg(column)</code> - the mean of (numeric) values in <code>column</code> for each group</li> <li><code>std(column)</code> - the standard deviation of (numeric) values in <code>column</code> for each group</li> <li><code>var(column)</code> - the variance of (numeric) values in <code>column</code> for each group</li> <li><code>agg(column,separator)</code> - the values of <code>column</code> in each group are concatenated, delimited by <code>separator</code> (default <code>separator</code> is none)</li> <li><code>json_array_agg(column,[str])</code> - the values of <code>column</code> in each group are concatenated into a JSON array (<code>[1,2,3]</code>). If the optional <code>str</code> argument is provided, the values in the array are quoted (<code>[\"1\", \"2\", \"3\"]</code>)</li> </ul> <p>Numeric aggregation functions will fail with errors if used on non-numeric column values.</p> <p>Note the difference between <code>min()</code>/<code>max()</code> and <code>str_min()</code>/<code>str_max()</code>: given a list like <code>10, 11, 98, 99, 100, 101</code>, return values are</p> function return <code>min()</code> 10 <code>str_min()</code> 10 <code>max()</code> 101 <code>str_max()</code> 99"},{"location":"configuration/#debug-operation","title":"Debug operation","text":"debug <p>Prints out information about the data at the current transformation operation.</p> <pre><code>    - operation: debug\n        function: head # or `tail`, `describe`, `columns`; default=`head`\n        rows: 10 # (optional, default=5; ignored if function=describe|columns)\n        transpose: True # (default=False; ignored when function=columns)\n        skip_columns: [a, b, c] # to avoid logging PII\n        keep_columns: [x, y, z] # to look only at specific columns\n</code></pre> <ul> <li><code>function=head|tail</code> displays the <code>rows</code> first or last rows of the dataframe, respectively. (Note that on large dataframes, these may not truly be the first/last rows, due to Dask's memory optimizations.)</li> <li><code>function=describe</code> shows statistics about the values in the dataframe.</li> <li><code>function=columns</code> shows the column names in the dataframe.</li> <li><code>transpose</code> can be helpful with very wide dataframes.</li> <li><code>keep_columns</code> defaults to all columns, <code>skip_columns</code> defaults to no columns.</li> </ul>"},{"location":"configuration/#destinations","title":"<code>destinations</code>","text":"<p>The <code>destinations</code> section specifies how transformed data is materialized to files.</p> <p>A sample destinations section is shown here; the options are explained below.</p> earthmover.yml<pre><code>destinations:\n  schools:\n    source: $transformations.school_list\n    template: ./json_templates/school.jsont\n    extension: jsonl\n    linearize: True\n  courses:\n    source: $transformations.course_list\n    template: ./json_templates/course.jsont\n    extension: jsonl\n    linearize: True\n  course_report:\n    source: $transformations.course_list\n    template: ./json_templates/course.htmlt\n    extension: html\n    linearize: False\n    header: &lt;html&gt;&lt;body&gt;&lt;h1&gt;Course List:&lt;/h1&gt;\n    footer: &lt;/body&gt;&lt;/html&gt;\n</code></pre> <p>For each file you want materialized, provide the <code>source</code> and (optionally) the Jinja <code>template</code> file. The materialized file will contain <code>template</code> rendered for each row of source, with an optional <code>header</code> prefix and <code>footer</code> postfix (both of which may contain Jinja, and which may reference <code>__row_data__</code> which is the first row of the data frame... a formulation such as <code>{%raw%}{% for k in __row_data__.pop('__row_data__').keys() %}{{k}}{% if not loop.last %},{% endif %}{% endfor %}{%endraw%}</code> may be useful). Files are materialized using your specified <code>extension</code> (which is required).</p> <p>If <code>linearize</code> is <code>True</code>, all line breaks are removed from the template, resulting in one output line per row. (This is useful for creating JSONL and other linear output formats.) If omitted, <code>linearize</code> is <code>True</code>.</p>"},{"location":"configuration/#global-options","title":"Global options","text":"<p>Any <code>source</code>, <code>transformation</code>, or <code>destination</code> node may additionally specify</p> <ul> <li><code>debug: True</code>, which outputs the dataframe shape and columns after the node completes processing (this can be helpful for building and debugging)</li> <li><code>require_rows: True</code> or <code>require_rows: 10</code> to have earthmover exit with an error if <code>0</code> (for <code>True</code>) or <code>&lt;10</code> (for <code>10</code>) rows are present in the dataframe after the node completes processing</li> <li><code>show_progress: True</code> to display a progress bar while processing this node</li> <li><code>repartition: True</code> to repartition the node in memory before continuing to the next node; set either the number of bytes, or a text representation (e.g., \"100MB\") to shuffle data into new partitions of that size (Note: this configuration is advanced, and its use may drastically affect performance)</li> </ul>"},{"location":"configuration/#jinja-templates","title":"Jinja templates","text":"<p><code>earthmover</code> destinations can specify a Jinja template to render for each row of the final dataframe \u2014 a text file (JSON, XML, HTML, etc.) containing Jinja with references to the columns of the row.</p> <p>An example Jinja template for JSON could be my_template.jsont<pre><code>{\n  \"studentUniqueId\": \"{{student_id}}\",\n  \"birthDate\": \"{{birth_date}}\",\n  \"firstName\": \"{{first_name}}\",\n  \"lastSurname\": \"{{last_name}}\"\n}\n</code></pre></p> <p>Using a linter on the output of <code>earthmover run</code> can help catch syntax mistakes when developing Jinja templates.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Development of earthmover is done via GitHub:</p> <ul> <li>Bugs may be reported via Issues. <code>earthmover</code>'s maintainers periodically review issues.</li> <li>Bugfixes and new features (such as additional transformation operations) are gratefully accepted via pull requests. Branch protection is enabled for the repo, so please make your pull requests from a fork of the repo.</li> </ul>"},{"location":"design/","title":"Design","text":"<p>This page discusses some aspects of <code>earthmover</code>'s design.</p>"},{"location":"design/#yaml-compilation","title":"YAML compilation","text":"<p><code>earthmover</code> allows Jinja templating expressions in its YAML configuration files. (This is similar to how Ansible Playbooks work.) <code>earthmover</code> parses the YAML in several steps:</p> <ol> <li>Extract only the <code>config</code> section (if any), in order to make available any <code>macros</code> when parsing the rest of the Jinja + YAML. The <code>config</code> section only may not contain any Jinja (besides <code>macros</code>).</li> <li>Load the entire Jinja + YAML as a string and hydrate all parameter references.</li> <li>Parse the hydrated Jinja + YAML string with any <code>macros</code> to plain YAML.</li> <li>Load the plain YAML string as a nested dictionary and begin building and processing the DAG.</li> </ol> <p>Note that due to step (3) above, runtime Jinja expressions (such as column definitions for <code>add_columns</code> or <code>modify_columns</code> operations) should be wrapped with <code>{%raw%}...{%endraw%}</code> to avoid being parsed when the YAML is being loaded.</p> <p>The parsed YAML is written to a file called <code>earthmover_compiled.yaml</code> in your working directory during a <code>compile</code> command. This file can be used to debug issues related to compile-time Jinja or project composition.</p>"},{"location":"design/#data-dependency-graph-dag","title":"Data dependency graph (DAG)","text":"<p><code>earthmover</code> models the <code>sources</code> \u2192 <code>transformations</code> \u2192 <code>destinations</code> data flow as a directed acyclic graph (DAG). <code>earthmover</code> will raise an error if your YAML configuration is not a valid DAG (if, for example, it contains a cycle).</p> <p>Each component of the DAG is run separately.</p> <p></p> <p>Each component is materialized in topological order. This minimizes memory usage, as only the data from the current and previous layer must be retained in memory.</p> <p></p> <p>Tip: visualize an earthmover DAG</p> <p>Setting <code>config</code> \u00bb <code>show_graph: True</code> will make <code>earthmover run</code> produce a visualization of the DAG for a project, such as</p> <p></p> <p>In this diagram:</p> <ul> <li>Green nodes on the left correspond to <code>sources</code></li> <li>Blue nodes in the middle correspond to <code>transformations</code></li> <li>Red nodes on the right correspond to <code>destinations</code></li> <li><code>sources</code> and <code>destinations</code> are annotated with the file size (in Bytes)</li> <li>all nodes are annotated with the number of rows and columns of data at that step</li> </ul>"},{"location":"design/#dataframes","title":"Dataframes","text":"<p>All data processing is done using Pandas Dataframes and Dask, with values stored as strings (or Categoricals, for memory efficiency in columns with few unique values). This choice of datatypes prevents issues arising from Pandas' datatype inference (like inferring numbers as dates), but does require casting string-representations of numeric values using Jinja when doing comparisons or computations.</p>"},{"location":"design/#performance","title":"Performance","text":"<p>Tool performance depends on a variety of factors including source file size and/or database performance, the system's storage performance (HDD vs. SSD), memory, and transformation complexity. But some effort has been made to engineer this tool for high throughput and to work in memory- and compute-constrained environments.</p> <p>Smaller source data (which all fits into memory) processes very quickly. Larger chunked sources are necessarily slower. We have tested with sources files of 3.3GB, 100M rows (synthetic attendance data): creating 100M lines of JSONL (30GB) takes around 50 minutes on a modern laptop.</p> <p>The state feature adds some overhead, as hashes of input data and JSON payloads must be computed and stored, but this can be disabled if desired.</p>"},{"location":"design/#comparison-to-dbt","title":"Comparison to <code>dbt</code>","text":"<p><code>earthmover</code> is similar in a number of ways to <code>dbt</code>. Some ways in which the tools are similar include...</p> <ul> <li>both are open-source data transformation tools</li> <li>both use YAML project configuration</li> <li>both manage data dependencies as a DAG</li> <li>both manage data transformation as code</li> <li>both support packages (reusable data transformation modules)</li> </ul> <p>But there are some significant differences between the tools too, including...</p> <ul> <li><code>earthmover</code> runs data transformations locally, while <code>dbt</code> issues SQL transformation queries to a database engine for execution. (For database <code>sources</code>, <code>earthmover</code> downloads the data from the database and processes it locally.)</li> <li><code>earthmover</code>'s data transformation instructions are <code>operations</code> expressed as YAML, while <code>dbt</code>'s transformation instructions are (Jinja-templated) SQL queries.</li> </ul> <p>The team that maintains <code>earthmover</code> also uses (and loves!) <code>dbt</code>. Our data engineers typically use <code>dbt</code> for large datasets (GB+) in a cloud database (like Snowflake) and <code>earthmover</code> for smaller datasets (&lt; GB) in files (CSV, etc.).</p>"},{"location":"learn/","title":"Learn","text":"<p>This page contains resources that may be helpful for learning to use <code>earthmover</code>. Using <code>earthmover</code> will be easier if you understand these topics.</p>"},{"location":"learn/#yaml","title":"YAML","text":"<p>YAML is a popular format for storing data in various structures. It is functionally equivalent to JSON.</p> <p>This video is a good introduction to YAML. For <code>earthmover</code>, it is helpful to be familiar with comments, basic data types, lists and dictionaries, nesting, and anchors, aliases, and overrides. </p> Did you know? <p>YAML was chosen as the configuration language for <code>earthmover</code> because it's less verbose than XML or JSON, and it supports comments!</p> YAML example<pre><code># YAML supports comments!\n\n# scalar values\nmy_boolean: True # or False\nmy_integer: 47\nmy_float: 3.14\nmy_string: string_value\nanother_string: \"Another string value!\"\n\n# lists (arrays)\nmy_small_number_array: [1, 2, 3]\nmy_large_quote_array:\n  - \"The only thing we have to fear is fear itself\"\n  - \"I think, therefore I am\"\n  - \"Simplicity is the ultimate sophistication\"\n  - \"In the middle of every difficulty lies opportunity\"\n  - \"The secret of getting ahead is getting started\"\n  - \"There is no substitute for hard work\"\n  - \"Whatever you do, do it well\"\n  - \"The best way to predict the future is to create it\"\n\n# dictionaries (objects)\nmy_car:\n  make: Subaru\n  model: Outback\n  year: 2015\n  color: blue\n  plate: N0 WAY\n\n# lists of objects\nmy_pets:\n- type: dog\n  name: Snoopy\n  age: 7\n  is_good_boy: True\n- type: cat\n  name: Garfield\n  age: 6\n  has_attitude: True\n</code></pre>"},{"location":"learn/#jinja-templates","title":"Jinja templates","text":"<p>This video is a good introduction to Jinja templates. For <code>earthmover</code>, it is helpful to understand Jinja comments, variables, control structures like if and for, filters, and macros.</p> Did you know? <p>Jinja and YAML have emerged as de-facto languages in data engineering and other engineering disciplines. <code>ansible</code> (a devops tool) uses YAML that may contain Jinja, just like <code>earthmover</code>. <code>dbt</code> uses plain YAML for its project configuration with Jinja in the SQL models it runs.</p> Jinja example<pre><code>{# Jinja supports comments! #}\n\nHello {{user.name}}! You were born in {{user.birth_date[0:4]}}.\n\n{% if user.pets | length &gt; 0 %}\n  Your pets include:\n  {% for pet in user.pets %}\n    * {{pet.name}} ({{pet.age}}) {% if pet.is_good_boy %}\ud83d\udc36{% endif %}\n  {% endfor %}\n  {% if user.pets | selectattr(\"has_attitude\") | length &gt; 0 %}\n    Some of your pets have attitude!\n  {% endif %}\n{% else %}\n  You have no pets \ud83d\ude1e (yet!)\n{% endif %}\n</code></pre>"},{"location":"license/","title":"License","text":"<p><code>earthmover</code> is open source software released under the Apache 2 license.</p>"},{"location":"packages/","title":"Packages","text":"<p>Packages are <code>earthmover</code> projects that are designed to be shared, reused, and built upon. They may contain seed data, template files, and an <code>earthmover.yml</code> with <code>sources</code>, <code>transformations</code>, and <code>destinations</code> that define a (potentially partial) data transformation graph, as well as a README explaining how to use the package.</p> <p>Here we maintain a list of packages for various domain-specific uses:</p> <ul> <li>Packages for transforming assessment data files from various vendors to the Ed-Fi data standard</li> </ul> <p>If you develop packages, please consider contributing them to the community by publishing them online and emailing the link to treitz@edanalytics.org to get them listed above.</p>"},{"location":"practices/","title":"Best Practices","text":"<p>In this section we outline some suggestions for best practices to follow when using <code>earthmover</code>, based on our experience with the tool. Many of these are based on best practices for using dbt, to which <code>earthmover</code> is similar, although <code>earthmover</code> operates on dataframes rather than database tables.</p>"},{"location":"practices/#project-structure","title":"Project structure","text":"<p>A typical <code>earthmover</code> project might have a structure like this: <pre><code>project/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 sources/\n\u2502   \u2514\u2500\u2500 source_file_1.csv\n\u2502   \u2514\u2500\u2500 source_file_2.csv\n\u2502   \u2514\u2500\u2500 source_file_3.csv\n\u251c\u2500\u2500 earthmover.yaml\n\u251c\u2500\u2500 output/\n\u2502   \u2514\u2500\u2500 output_file_1.jsonl\n\u2502   \u2514\u2500\u2500 output_file_2.xml\n\u251c\u2500\u2500 seeds/\n\u2502   \u2514\u2500\u2500 crosswalk_1.csv\n\u2502   \u2514\u2500\u2500 crosswalk_2.csv\n\u251c\u2500\u2500 templates/\n\u2502   \u2514\u2500\u2500 json_template_1.jsont\n\u2502   \u2514\u2500\u2500 json_template_2.jsont\n\u2502   \u2514\u2500\u2500 xml_template_1.xmlt\n\u2502   \u2514\u2500\u2500 xml_template_2.xmlt\n</code></pre> The mappings, transformations, and structure of your data \u2013 which are probably not sensitive \u2013 should generally be separated from the actual input and output \u2013 which may be large and/or sensitive, and therefore should not be committed to a version control system. This can be accomplished in two ways:</p> <ol> <li>include a <code>.gitignore</code> or similar file in your project which excludes the <code>sources/</code> and <code>output/</code> directories from being committed the repository</li> <li>remove the <code>sources/</code> and <code>output/</code> directories from your project and update <code>earthmover.yaml</code>'s <code>sources</code> and <code>destinations</code> to reference another location outside the <code>project/</code> directory</li> </ol> <p>When dealing with sensitive source data, you may have to comply with security protocols, such as referencing sensitive data from a network storage location rather than copying it to your own computer. In this situation, option 2 above is a good choice.</p> <p>To facilitate operationalization, we recommended using relative paths from the location of the <code>earthmover.yaml</code> file and using parameters to pass dynamic filenames to <code>earthmover</code>, instead of hard-coding them. For example, rather than <pre><code>config:\n  output_dir: /path/to/outputs/\n...\nsources:\n  source_1:\n    file: /path/to/inputs/source_file_1.csv\n    header_rows: 1\n  source_2:\n    file: /path/to/inputs/source_file_2.csv\n    header_rows: 1\n  seed_1:\n    file: /path/to/seeds/seed_1.csv\n...\ndestinations:\n  output_1:\n    source: $transformations.transformed_1\n    ...\n  output_2:\n    source: $transformations.transformed_2\n    ...\n</code></pre> instead consider using <pre><code>config:\n  output_dir: ${OUTPUT_DIR}\n...\nsources:\n  source_1:\n    file: ${INPUT_FILE_1}\n    header_rows: 1\n  source_2:\n    file: ${INPUT_FILE_2}\n    header_rows: 1\n  seed_1:\n    file: ./seeds/seed_1.csv\n...\ndestinations:\n  output_1:\n    source: $transformations.transformed_1\n    ...\n  output_2:\n    source: $transformations.transformed_2\n    ...\n</code></pre> and then run with <pre><code>earthmover -p '{ \"OUTPUT_DIR\": \"/path/to/outputs/\", \\\n\"INPUT_FILE_1\": \"/path/to/source_file_1.csv\", \\\n\"INPUT_FILE_2\": \"/path/to/source_file_2.csv\" }'\n</code></pre> Note that with this pattern you can also use optional sources to only create one of the outputs if needed, for example <pre><code>earthmover -p '{ \"OUTPUT_DIR\": \"/path/to/outputs/\", \\\n\"INPUT_FILE_1\": \"/path/to/source_file_1.csv\" }'\n</code></pre> would only create <code>output_1</code> if <code>source_1</code> had <code>required: False</code> (since <code>INPUT_FILE_2</code> is missing).</p>"},{"location":"practices/#development-practices","title":"Development practices","text":"<p>While YAML is a data format, it is best to treat the <code>earthmover</code> YAML configuration as code, meaning you should</p> <ul> <li>version it!</li> <li>avoid putting credentials and other sensitive information in the configuration; rather specify such values as parameters</li> <li>keep your YAML DRY by using Jinja macros and YAML anchors and aliases</li> </ul> <p>Remember that code is poetry: it should be beautiful! To that end</p> <ul> <li>Carefully choose concise, good names for your <code>sources</code>, <code>transformations</code>, and <code>destinations</code>.<ul> <li>Good names for <code>sources</code> could be based on their source file/table (e.g. <code>students</code> for <code>students.csv</code>)</li> <li>Good names for <code>transformations</code> indicate what they do (e.g. <code>students_with_mailing_addresses</code>)</li> <li>Good names for <code>destinations</code> could be based on the destination file (e.g. <code>student_mail_merge.xml</code>)</li> </ul> </li> <li>Add good, descriptive comments throughout your YAML explaining any assumptions or non-intuitive operations (including complex Jinja).</li> <li>Likewise put Jinja comments in your templates, explaining any complex logic and structures.</li> <li>Keep YAML concise by consolidating <code>transformation</code> operations where possible. Many operations like <code>add_columns</code>, <code>map_values</code>, and others can operate on multiple <code>columns</code> in a dataframe.</li> <li>At the same time, avoid doing too much at once in a single <code>transformation</code>; splitting multiple <code>join</code> operations into separate transformations can make debugging easier.</li> </ul>"},{"location":"practices/#debugging-practices","title":"Debugging practices","text":"<p>When developing your transformations, it can be helpful to</p> <ul> <li>specify <code>config</code> \u00bb <code>log_level: DEBUG</code> and <code>transformation</code> \u00bb <code>operation</code> \u00bb <code>debug: True</code> to verify the columns and shape of your data after each <code>operation</code></li> <li>turn on <code>config</code> \u00bb <code>show_stacktrace: True</code> to get more detailed error messages</li> <li>avoid name-sharing for a <code>source</code>, a <code>transformation</code>, and/or a <code>destination</code> - this is allowed but can make debugging confusing</li> <li>install pygraphviz and turn on <code>config</code> \u00bb <code>show_graph: True</code>, then visually inspect your transformations in <code>graph.png</code> for structural errors</li> <li>use a linter/validator to validate the formatting of your generated data</li> </ul> <p>You can remove these settings once your <code>earthmover</code> project is ready for operationalization.</p>"},{"location":"practices/#operationalization-practices","title":"Operationalization practices","text":"<p>Typically <code>earthmover</code> is used when the same (or similar) data transformations must be done repeatedly. (A one-time data transformation task may be more easily done with SQLite or a similar tool.) When deploying/operationalizing <code>earthmover</code>, whether with a simple scheduler like cron or an orchestration tool like Airflow or Dagster, consider</p> <ul> <li>specifying conditions you <code>expect</code> your sources to meet, so <code>earthmover</code> will fail on source data errors</li> <li> <p>specifying <code>config</code> \u00bb <code>log_level: INFO</code> and monitoring logs for phrases like</p> <p><code>distinct_rows</code> operation removed NN duplicate rows</p> <p><code>filter_rows</code> operation removed NN rows</p> </li> <li> <p>using the structured run output flag and shipping the output somewhere it can be queried or drive a monitoring dashboard</p> </li> </ul>"},{"location":"setup/","title":"Setup","text":"<p>To use <code>earthmover</code>, your system must have Python 3 and pip installed.</p> <p>Then <pre><code>pip install earthmover\n</code></pre> which will install <code>earthmover</code> and the packages it depends on.</p> <p>Verify that <code>earthmover</code> has successfully installed with <pre><code>earthmover -v\n</code></pre></p> <p>Next, configure your project, or <code>earthmover init</code> to initialize a sample project.</p>"},{"location":"usage/","title":"Usage","text":"<p>This page explains how to use <code>earthmover</code>: the commands and features available in the tool.</p>"},{"location":"usage/#commands","title":"Commands","text":"<p>As a command-line tool, <code>earthmover</code> supports a number of different commands, which are documented below. (Simply running <code>earthmover</code> without specifying a command is equivalent to <code>earthmover run</code>.)</p>"},{"location":"usage/#earthmover-init","title":"<code>earthmover init</code>","text":"<p><code>earthmover init</code> creates a simple <code>earthmover</code> project skeleton in the current directory with</p> <ul> <li>several simple source data files</li> <li>an <code>earthmover.yml</code> file</li> <li>a sample Jinja template</li> </ul> <p>which can be a useful starting point for learning about <code>earthmover</code> or developing a new <code>earthmover</code> project.</p>"},{"location":"usage/#earthmover-deps","title":"<code>earthmover deps</code>","text":"<p>(This command is only needed if your <code>earthmover</code> project depends on <code>packages</code>.) Running <code>earthmover deps</code> will download and install the <code>packages</code> in a <code>packages/</code> folder. This step must be completed before you can <code>earthmover compile</code> or <code>earthmover run</code>.</p>"},{"location":"usage/#earthmover-compile","title":"<code>earthmover compile</code>","text":"<p><code>earthmover compile</code> is an optional step that can help validate an <code>earthmover.yml</code> configuration file. Running it renders any Jinja and parameters, then parses the resulting YAML into a data dependency graph. This helps identify any compile-time errors before doing any actual data transformation.</p>"},{"location":"usage/#earthmover-run","title":"<code>earthmover run</code>","text":"<p><code>earthmover run</code> compiles an <code>earthmover</code> project and then executes the data dependency graph (in topological order).</p> <p>Flags that may be used with <code>earthmover run</code> include</p> <ul> <li> <p><code>-c</code> to specify a YAML configuration file <pre><code>earthmover run -c path/to/config.yaml\n</code></pre>     (If omitted, earthmover looks for <code>earthmover.yml</code> or <code>earthmover.yaml</code> in the current directory.)</p> </li> <li> <p><code>--set</code> to override values specified in the YAML configuration file, for example     <pre><code>earthmover run --set config.tmp_dir path/to/another/dir/\nearthmover run --set sources.schools.file './my schools with spaces.csv'\nearthmover run --set destinations.my_dest.extension ndjson destinations.my_dest.linearize True\n</code></pre>     (<code>--set</code> must be followed by a set of key-value pairs.)</p> </li> </ul>"},{"location":"usage/#earthmover-clean","title":"<code>earthmover clean</code>","text":"<p><code>earthmover clean</code> removes data files created by <code>earthmover</code> (those in <code>config.output_dir</code>), and, if it exists, <code>earthmover_compiled.yml</code>.</p>"},{"location":"usage/#earthmover-v","title":"<code>earthmover -v</code>","text":"<p>See the currently-installed version of <code>earthmover</code> with <pre><code>earthmover -v\n# or\nearthmover --version\n</code></pre></p>"},{"location":"usage/#earthmover-h","title":"<code>earthmover -h</code>","text":"<p>See a CLI help message with <pre><code>earthmover -h\n# or\nearthmover --help\n</code></pre></p>"},{"location":"usage/#earthmover-t","title":"<code>earthmover -t</code>","text":"<p><code>earthmover</code> ships with a test suite covering all transformation operations. It can be run with <pre><code>earthmover -t\n</code></pre> which simply runs the tool on the <code>earthmover.yaml</code> and toy data in the <code>earthmover/tests/</code> folder. (The DAG is pictured below.) Rendered <code>earthmover/tests/output/</code> are then compared against the <code>earthmover/tests/expected/</code> output; the test passes only if all files match exactly.</p> <p></p>"},{"location":"usage/#features","title":"Features","text":""},{"location":"usage/#state","title":"State","text":"<p><code>earthmover</code> can maintain state about past runs, and only re-process subsequent runs if relevant data or configuration has changed \u2014 the YAML configuration itself, data <code>file</code>s of <code>sources</code>, the <code>map_file</code>(s) of a <code>map_values</code> transformation operation, <code>template</code> file(s) of <code>destinations</code>, or parameter values.</p> <p>State is tracked by hashing files and parameters; hashes and run timestamps are stored in the file specified by <code>config.state_file</code>. (If that file is not specified, state-tracking is disabled.)</p> <p>You may choose to force reprocessing of the whole DAG, regardless of whether has changed, using the <code>-f</code> or <code>--force-regenerate</code> command-line flag:</p> <pre><code>earthmover run -f\nearthmover run --force-regenerate\n</code></pre> <p>To further avoid computing hashes and not log a run to the <code>state_file</code>, use the <code>-k</code> or <code>--skip-hashing</code> flag:</p> <pre><code>earthmover run -k\nearthmover run --skip-hashing\n</code></pre> <p>(This makes a one-time run on large input files faster.)</p> <p>If earthmover skips running because state has not changed, it returns bash exit code 99 (this was chosen because it signals a \"skipped\" task in Airflow).</p>"},{"location":"usage/#parameters","title":"Parameters","text":"<p><code>earthmover</code> supports parameterization of the YAML configuration via named parameter-value pairs. Example: earthmover.yml<pre><code>sources:\n  people:\n    file: ${BASE_DIR}/people.csv\n    ...\n</code></pre></p> <p>Avoid parameters in single quotes under Windows</p> <p>Because <code>os.path.expandvars()</code> does not expand variables within single quotes in Python under Windows, Windows users should avoid placing parameter references inside single quote strings in their YAML.</p> <p>Parameter values may be specified via the CLI or as environment variables. <pre><code>earthmover run -p '{\"BASE_DIR\":\"path/to/my/base/dir\"}'\n# or\nearthmover run --params '{\"BASE_DIR\":\"path/to/my/base/dir\"}'\n# or\nexport BASE_DIR=path/to/my/base/dir\nearthmover run\n</code></pre> (Command-line parameters override any environment variables of the same name.)</p> <p>Default parameter values can be specified in the YAML configuration's <code>config</code> section, which is particularly useful when developing an <code>earthmover</code> project that is intended to be a package used by other projects.</p> earthmover.yml<pre><code>config:\n  parameter_defaults:\n    BASE_DIR: ~/earthmover/\n    ...\n</code></pre>"},{"location":"usage/#jinja-in-yaml-configuration","title":"Jinja in YAML configuration","text":"<p>You may use Jinja in the YAML configuration, which will be parsed at load time. Only the <code>config</code> section may not contain Jinja, except for <code>macros</code> which are made available both at parse-time for Jinja in the YAML configuration and at run-time for Jinja in <code>add_columns</code> or <code>modify_columns</code> transformation operations.</p> <p>The following example</p> <ol> <li>loads 9 source files</li> <li>adds a column indicating the source file each row came from</li> <li>unions the sources together</li> <li>if an environment variable or parameter <code>DO_FILTERING=True</code> is passed, filters out certain rows</li> </ol> <pre><code>config:\n  show_graph: True\n  parameter_defaults:\n    DO_FILTERING: \"False\"\n\nsources:\n{% for i in range(1,10) %}\n  source{{i}}:\n    file: ./sources/source{{i}}.csv\n    header_rows: 1\n{% endfor %}\n\ntransformations:\n{% for i in range(1,10) %}\n  source{{i}}:\n    source: $sources.source{{i}}\n    operations:\n      - operation: add_columns\n        columns:\n          source_file: {{i}}\n{% endfor %}\n  stacked:\n    source: $transformations.source1\n    operations:\n      - operation: union\n        sources:\n{% for i in range(2,10) %}\n          - $transformations.source{{i}}\n{% endfor %}\n{% if \"${DO_FILTERING}\"==\"True\" %}\n      - operations: filter_rows\n        query: school_year &lt; 2020\n        behavior: exclude\n{% endif %}\n\ndestinations:\n  final:\n    source: $transformations.stacked\n    template: ./json_templates/final.jsont\n    extension: jsonl\n    linearize: True\n</code></pre>"},{"location":"usage/#selectors","title":"Selectors","text":"<p>Run only portions of the data dependency graph (DAG) by using a selector: <pre><code>earthmover run -s people,people_*\n</code></pre> This processes all DAG paths (from <code>sources</code> through <code>destinations</code>) through any matched nodes.</p>"},{"location":"usage/#optional-sources","title":"Optional Sources","text":"<p>If you specify the <code>columns</code> list and <code>optional: True</code> on a file <code>source</code> but leave the <code>file</code> blank, <code>earthmover</code> will create an empty dataframe with the specified columns and pass it through the rest of the DAG. This, combined with the use of parameters to specify a <code>source</code>'s <code>file</code>, provides flexibility to include data when it's available but still run when it is missing.</p> earthmover.yml<pre><code>sources:\n  my_optional_source:\n    file: \"\"\n    columnns:\n      - id\n      - year\n      - code\n      - ...\n</code></pre>"},{"location":"usage/#structured-run-output","title":"Structured run output","text":"<p>To produce a JSON file with metadata about the run, invoke earthmover with</p> <p><pre><code>earthmover run -c path/to/earthmover.yml --results-file ./results.json\n</code></pre> For example, for <code>example_projects/09_edfi/</code>, a sample results file would be: <pre><code>{\n    \"started_at\": \"2023-06-08T10:21:42.445835\",\n    \"working_dir\": \"/home/someuser/code/repos/earthmover/example_projects/09_edfi\",\n    \"config_file\": \"./earthmover.yaml\",\n    \"output_dir\": \"./output/\",\n    \"row_counts\": {\n        \"$sources.schools\": 6,\n        \"$sources.students_anytown\": 1199,\n        \"$sources.students_someville\": 1199,\n        \"$destinations.schools\": 6,\n        \"$transformations.all_students\": 2398,\n        \"$destinations.students\": 2398,\n        \"$destinations.studentEducationOrganizationAssociations\": 2398,\n        \"$destinations.studentSchoolAssociations\": 2398\n    },\n    \"completed_at\": \"2023-06-08T10:21:43.118854\",\n    \"runtime_sec\": 0.673019\n}\n</code></pre></p>"},{"location":"usage/#project-composition","title":"Project Composition","text":"<p>An <code>earthmover</code> project can import and build upon other <code>earthmover</code> projects by importing them as packages, similar to the concept of dbt packages. When a project uses a package, any elements of the package can be overwritten by the project. This allows you to use majority of the code from a package and specify only the necessary changes in the project.</p> <p>To install the packages specified in your YAML Configuration, run <code>earthmover deps</code>. Packages will be installed in a nested format in a <code>packages/</code> directory. Once packages are installed, <code>earthmover</code> can be run as usual. If you make any changes to the packages, run <code>earthmover deps</code> again to install the latest version of the packages. </p>"},{"location":"usage/#composition-example","title":"Composition example","text":"<p>In the example below, an <code>earthmover</code> project <code>projA</code> depends on a package <code>projB</code>. Running <code>earthmover compile</code> on <code>projA/earthmover.yml</code> results in the <code>earthmover.yml</code> files from both projects being combined, with the result shown in <code>projA/earthmover_compiled.yml</code>.</p> <code>projA/earthmover.yml</code>+ <code>projA/pkgB/earthmover.yml</code>\u2192 composed <code>projA/earthmover_compiled.yml</code> <pre><code>config:\n  show_graph: True\n  output_dir: ./output\n\npackages:\n  pkgB:\n    local: pkgB\n\nsources:\n  source1:\n    file: ./seeds/source1.csv\n    header_rows: 1\n\ndestinations:\n  dest1:\n    source: $transformations.trans1\n    template: ./templates/dest1.jsont\n</code></pre> <pre><code>config:\n  parameter_defaults:\n    DO_FILTERING: \"False\"\nsources:\n  source1:\n    file: ./seeds/source1.csv\n    header_rows: 1\n  source2:\n    file: ./seeds/source2.csv\n    header_rows: 1\n\ntransformations:\n  trans1:\n    ...\ndestinations:\n  dest1:\n    source: $transformations.trans1\n    template: ./templates/dest1.jsont\n  dest2:\n    source: $sources.source2\n    template: ./templates/dest2.jsont\n</code></pre> <pre><code>config:\n  show_graph: True\n  output_dir: ./output \n  parameter_defaults:\n    DO_FILTERING: \"False\"\n\npackages:\n  pkgB:\n    local: pkgB\n\nsources:\n  source1:\n    file: ./seeds/source1.csv\n    header_rows: 1  \n  source2: \n    file: ./packages/pkgB/seeds/source2.csv\n    header_rows: 1    \n\ntransformations:\n  trans1:\n    ...\n\ndestinations:\n  dest1:\n    source: $transformations.trans1\n    template: ./templates/dest1.jsont\n  dest2:\n    source: $sources.source2\n    template: ./packages/pkgB/templates/dest2.jsont\n</code></pre>"},{"location":"usage/#composition-considerations","title":"Composition considerations","text":"<ul> <li> <p>The <code>config</code> section is not composed from the installed packages, with the exception of <code>macros</code> and <code>parameter_defaults</code>. Specify all desired configuration in the top-level project.</p> </li> <li> <p>There is no limit to the number of packages that can be imported and no limit to how deeply they can be nested (i.e. packages can import other packages). However, there are a few things to keep in mind with using multiple packages.</p> </li> <li>If multiple packages at the same level (e.g. <code>projA/packages/pkgB</code> and <code>projA/packages/pkgC</code>, not <code>projA/packages/pkgB/packages/pkgC</code>) include same-named nodes, the package specified later in the <code>packages</code> list will overwrite. If the node is also specified in the top-level project, its version of the node will overwrite as usual.</li> <li>A similar limitation exists for macros \u2013 a single definition of each macro will be applied everywhere in the project and packages using the same overwrite logic used for the nodes. When you are creating projects that are likely to be used as packages, consider including a namespace in the names of macros with more common operations, such as <code>assessment123_filter()</code> instead of the more generic <code>filter()</code>. </li> </ul>"}]}